#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Mar 13 12:16:46 2018

@author: Omar
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pykalman import KalmanFilter
from hmmlearn import hmm
import matplotlib
from hmmlearn.hmm import MultinomialHMM
from sklearn.preprocessing import normalize
data = pd.read_csv("/Users/Omar/Documents/Year4/machineLearning/coursework2/data/question3/Ethereum_Prices.csv", names = ["Open", "High", "Low", "Close", "Volume", "Time"])

#kf = KalmanFilter(n_dim_state=2,n_dim_obs=1)
#
#exp = kf.em(data["Open"])
#
#quant_means, quant_covs = kf.filter(data["Open"])
#
#plt.plot(quant_means[:,0])
#plt.plot(data["Open"])
#plt.show()
#
#


#
#
#model = hmm.GaussianHMM(n_components=3)
#model.fit(data["Open"])
#
#colours = []
#
## define the colormap
#cmap = plt.cm.jet
## extract all colors from the .jet map
#cmaplist = [cmap(i) for i in range(cmap.N)]
## create the new map
#cmap = cmap.from_list('Custom cmap', cmaplist, cmap.N)
#
#
#for i in range(1440):
#    if (data["difference_threshold"][i] == "same"):
#        colours.append("red")
#    elif (data["difference_threshold"][i] == "up"):
#            colours.append("green")
#    else:
#        colours.append("blue")
#            
#        


#def normalize(A, dim=None, precision=1e-9):
#    """
#    I replaced the original normalize function with the one given by the instructor on piazza, since
#    the original had a bug.
#    This function is adapted from Kevin Murphy's code for Machine Learning: a Probabilistic Perspective.
#
#    Make the entries of a (multidimensional) array sum to 1
#    A, z = normalize(A) normalize the whole array, where z is the normalizing constant
#    A, z = normalize(A, dim)
#    If dim is specified, we normalize the specified dimension only.
#    dim=0 means each column sums to one
#    dim=1 means each row sums to one
#
#
#    Set any zeros to one before dividing.
#    This is valid, since s=0 iff all A(i)=0, so
#    we will get 0/1=0
#
#    Adapted from https://github.com/probml/pmtk3"""
#    
#    if dim is not None and dim > 1:
#        raise ValueError("Normalize doesn't support more than two dimensions.")
#    
#    z = A.sum(dim)
#    # If z is a scalar, z.shape is an empty tuple and evaluates to False
#    if z.shape:
#        z[np.abs(z) < precision] = 1
#    elif np.abs(z) < precision:
#        return 0, 1
#    
#    if dim == 1:
#        return np.transpose(A.T / z), z
#    else:
#        return A / z, z

def computeSmallB_Discrete(Y, B):
    """Compute the probabilities for the data points Y for a multinomial observation model 
        with observation matrix B
        
        Input parameters:
            - Y: the data
            - B: matrix of observation probabilities
        Output:
            - b: vector of observation probabilities
    """
    # initialise variables
    Nhidden = B.shape[0]
    T = len(Y)
    b = np.zeros((Nhidden,T))
    
    # simply select the appropriate values from the emission matrix B
    b[:,:] = B[:,Y[:]]
    
    return b

def BackwardFiltering(A, b, N, T):
    """Perform backward filtering.
        Input parameters:
            - A: estimated transition matrix (between states)
            - b: estimated observation probabilities (local evidence vector)
            - N: number of hidden states
            - T: length of the sequence
        Output:
            - beta: filtered probabilities
    """
    
    # initialise the shape of the entire beta matrix
    beta = np.zeros((N,T))
    # set beta_T
    beta[:,T-1] = np.array([1,1,1])
    
    # iterate backwards and find beta_t for t = 1,...,T-1
    for t in reversed(range(0,T-1)):
        beta[:,t] = A @ (b[:,t+1] * beta[:,t+1])
    
    return beta

def ForwardFiltering(A, b, pi, N, T):
    """Filtering using the forward algorithm (Section 17.4.2 of K. Murphy's book)
    Input:
      - A: estimated transition matrix
      - b: estimated observation probabilities (local evidence vector)
      - pi: initial state distribution pi(j) = p(z_1 = j)
      - N: number of hidden states
      - T: length of the sequence
      
    Output:
      - Filtered belief state at time t: alpha = p(z_t|x_1:t)
      - log p(x_1:T)
      - Z: normalization constant"""
    
    # initialise alpha, Z
    alpha = np.zeros((N,T))
    Z = np.zeros((T))
    
    # The below algorithm is given exactly by the pdf notes
    normalized = normalize((b[:,0] * pi[:,0]).reshape(-1,1),axis=0,norm='l1',return_norm=True)
    alpha[:,0], Z[0] = normalized[0].reshape(3),normalized[1]
    
    for t in range(1,T):
        normalized = normalize((b[:,t] * np.matmul(np.transpose(A),alpha[:,t-1] )).reshape(-1,1),norm='l1',return_norm=True,axis=0)
        alpha[:,t], Z[t] = normalized[0].reshape(3),normalized[1]
    
    # definition of the logProb
    logProb = np.sum(np.log(Z))
    
    return alpha, logProb, Z

def ForwardBackwardSmoothing(A, b, pi, N, T):
    """Smoothing using the forward-backward algorithm.
    Input:
      - A: estimated transition matrix
      - b: local evidence vector (observation probabilities)
      - pi: initial distribution of states
      - N: number of hidden states
      - T: length of the sequence
    Output:
      - alpha: filtered belief state as defined in ForwardFiltering
      - beta: conditional likelihood of future evidence as defined in BackwardFiltering
      - gamma: gamma_t(j) proportional to alpha_t(j) * beta_t(j)
      - lp: log probability defined in ForwardFiltering
      - Z: constant defined in ForwardFiltering"""

    # forward filter, then backward filter, then normalize.
    alpha, logProb, Z = ForwardFiltering(A,b,pi,N,T)
    beta = BackwardFiltering(A,b,N,T)
    gamma, norm = normalize(alpha * beta,axis=0,norm='l1',return_norm=True)
    
    return alpha, beta, gamma, logProb, Z


def SmoothedMarginals(A, b, alpha, beta, T, Nhidden):
    """ 
    Adapted as per the discussion on Piazza
    """
    "Two-sliced smoothed marginals p(z_t = i, z_t+1 = j | x_1:T)"
    
    marginal = np.zeros((Nhidden, Nhidden, T-1));

    for t in range(T-1):
        marginal[:, :, t] = normalize(A * np.outer(alpha[:, t], np.transpose( (b[:, t+1] * beta[:, t+1]) ) ),norm='l1',return_norm=True)[0];
    
    return marginal


def EM_estimate_multinomial(Y, Nhidden, Niter, epsilon, init):
    
    # Dimensions of the data
    N, T = Y.shape
    
    # Initialization
    
    # Initial transition matrix should be stochastic (rows sum to 1)
    A = init["A"]
    
    # Observation matrix B
    B = init["B"]
    
    # Class prior
    pi = init["pi"]
    
    ###############################################
    # EM algorithm
    
    i = 0
    # Initialize convergence criterion here
    # calculate the difference in the logProbability at each iteration. If this is 
    # less than epsilon then we assume we've converged to the local maximum.
    logProbOld = -100000
    logProbDiff = epsilon + 1
    
    
    while (i<Niter) and (logProbDiff > epsilon) : # and condition on criterion and precision epsilon
        # Iterate here
        
        

        gamma_all_l = np.zeros((N,Nhidden,T))
        epsilon_all_l = np.zeros((N,Nhidden, Nhidden, T-1))

        # EXPECTATION:
        # iterate through all the sequences, and compute the responsibilities and smoothed marginals
        b = computeSmallB_Discrete(Y[0], B)
        ### expectation step
        alpha, beta, gamma, logProbNew, Z = ForwardBackwardSmoothing(A, b, pi, Nhidden, T)
        gamma_all_l[0] = gamma
        epsilon_all_l[0] = SmoothedMarginals(A, b, alpha, beta, T, Nhidden)
    
        
        # MAXIMISATION:
        # compute pi, A as per the before formulas.
        for k in range(Nhidden):
            pi[k] = np.sum(gamma_all_l[0,k,1]) / np.sum(gamma_all_l[0,:,1])
            for j in range(Nhidden):
                A[j,k] = np.sum(epsilon_all_l[:,j,k,:])
            
        # normalise A
        A,_ = normalize(A,axis=1,norm='l1',return_norm=True) 
        
        # maximisation step for B:
        # use the suggested one hot encoding and then the formula derived.
        Bnew = np.zeros((Nhidden,3))
        for l in range(N):
            X = np.zeros((T,3))
            for m in range(T):
                X[m,Y[l,m]] =1
            Bnew = Bnew + np.matmul(gamma_all_l[l],X)
        
        B,_ = normalize(Bnew,axis=1,norm='l1',return_norm=True) # normalise B across rows. 
        # update convergence criteria 
        i+=1
        logProbDiff = logProbNew - logProbOld
        logProbOld = logProbNew
        
    return A, B, pi


def multiHMMtest():
    
    # train 
    train = data["difference_threshold"][0:200]
    train = train.reshape(1,200)
    # initialise parameters somehow
    init = {}
    
    A = np.array([[0.1,0.45,0.45],[0.1,0.6,0.3],[0.1,0.3,0.6]])
    B = np.array([[0.5,0.25,0.25],[0.25,0.5,0.5],[0.25,0.25,0.5]])
    pi = np.array([[0.1],[0.45],[0.45]])
    
    init["A"] = A
    init["B"] = B
    init["pi"] = pi
    
    A,B,pi = EM_estimate_multinomial(train, 3, 100, 0.1, init)
    
    correct = 0
    incorrect = 0
    
    for i in range(200,1440):
        # predict 201th to 1440th values
        
        # initialise with last values
        init = {}
        init["A"] = A
        init["B"] = B
        init["pi"] = pi

        # perform EM on all previous values (not including i)
        A,B,pi= EM_estimate_multinomial(data["difference_threshold"][0:i].reshape(1,-1),
                                        3, 100, 0.1, init)
        
        
        
        
        Ydummy0 = np.append(data["difference_threshold"][0:i].reshape(1,-1)[0],0).reshape(1,-1)
        Ydummy1 = np.append(data["difference_threshold"][0:i].reshape(1,-1)[0],1).reshape(1,-1)
        Ydummy2 = np.append(data["difference_threshold"][0:i].reshape(1,-1)[0],2).reshape(1,-1)
        
        
        b0 = computeSmallB_Discrete(Ydummy0[0], B)
        b1 = computeSmallB_Discrete(Ydummy1[0], B)
        b2 = computeSmallB_Discrete(Ydummy2[0], B)
        
        alpha0, logProb0, Z0 = ForwardFiltering(A,b0,pi,3,i+1)
        alpha1, logProb1, Z1 = ForwardFiltering(A,b1,pi,3,i+1)
        alpha2, logProb2, Z2 = ForwardFiltering(A,b2,pi,3,i+1)
        
        # make prediction based on whichever logProb is highest.
        
        prediction = np.argmax(np.array([logProb0,logProb1,logProb2]))
        print("prediction is:")
        print(prediction)
        print("actual is:")
        print(data["difference_threshold"][i])
        
        if prediction == data["difference_threshold"][i]:
            correct = correct + 1
        else:
            incorrect = incorrect + 1
        i = i+1
if __name__ == "__main__":
        # train 
        
        
    data = pd.read_csv("/Users/Omar/Documents/Year4/machineLearning/coursework2/data/question3/Ethereum_Prices.csv", names = ["Open", "High", "Low", "Close", "Volume", "Time"])
        
    data["Difference"] = np.zeros(1440)
  
    for i in range(1,1439):    
        data.set_value(i,"Difference",data["Close"][i + 1] - data["Close"][i])

    test = np.where(data["Difference"] == 0,
                0,np.where(data["Difference"] > 0,1,np.where(data["Difference"],2,-1)))
    data["difference_threshold"] = test

        
    train = data["difference_threshold"][0:200]
    train = train.reshape(1,200)
    # initialise parameters somehow
    init = {}
    
    A = np.array([[0.1,0.45,0.45],[0.1,0.6,0.3],[0.1,0.3,0.6]])
    B = np.array([[0.5,0.25,0.25],[0.25,0.5,0.25],[0.25,0.25,0.5]])
    pi = np.array([[0.1],[0.45],[0.45]])
    
    init["A"] = A
    init["B"] = B
    init["pi"] = pi
    
    A,B,pi = EM_estimate_multinomial(train, 3, 100, 0.01, init)
