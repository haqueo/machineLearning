install.packages("e1071")
library(e1071)
data("HouseVotes84")
head(HouseVotes84)
source('~/Documents/Year4/DataScience/20Oct/script.R')
source('~/Documents/Year4/DataScience/20Oct/script.R')
plot(HouseVotes84[,2])
title(main="Votes cast for issue", xlab="vote", ylab="Number reps")
source('~/Documents/Year4/DataScience/20Oct/script.R')
install.packages("ggplot")
library(ggplot)
library(ggplot2)
source('~/Documents/Year4/DataScience/20Oct/script.R')
nrow(HouseVotes84)
source('~/Documents/Year4/DataScience/20Oct/script.R')
source('~/Documents/Year4/DataScience/20Oct/script.R')
source('~/Documents/Year4/DataScience/20Oct/script.R')
source('~/Documents/Year4/DataScience/20Oct/script.R')
source('~/Documents/Year4/DataScience/20Oct/script.R')
source('~/Documents/Year4/DataScience/20Oct/script.R')
?HouseVotes84
head(HouseVotes84)
size(HouseVotes84)
dim(HouseVotes84)
Alex <- 430
Alex <- 250
HouseVotes84[Alex,]
Name <- Alex
Name
source('~/Documents/Year4/DataScience/20Oct/ex4.R')
dim(HouseVotes84)[0]
dim(HouseVotes84)
dim(HouseVotes84)[1]
head(HouseVotes84)
HouseVotes84[Name,1]
max.component <- max(components(docnet.disrupt)$csize)
??adm
setwd("/Users/Omar/Documents/Year4/machineLearning/coursework1")
library(rARPACK)
library(philentropy)
# I load the raw csv's
faces.train.inputs <- read.csv("./2018_ML_Assessed_Coursework_1_Data/Faces_Train_Inputs.csv",head=FALSE)
faces.train.label <- read.csv("./2018_ML_Assessed_Coursework_1_Data/Faces_Train_Labels.csv",head=FALSE)
faces.test.inputs <- read.csv("./2018_ML_Assessed_Coursework_1_Data/Faces_Test_Inputs.csv",head=FALSE)
faces.test.label <- read.csv("./2018_ML_Assessed_Coursework_1_Data/Faces_Test_Labels.csv",head=FALSE)
# I turn the input values into a list of 320 matrices, each matrix a 112 x 92 value of pixels corresponding to each image
# .. I need to use lapply again on the result because apply gives the matrices in a weird form
faces.train.inputs.cleaned <- lapply(apply(X=faces.train.inputs, MARGIN=1, function(x) list(matrix(as.numeric(x), nrow = 112))), "[[", 1)
# Here I calculate the average face
avg.face <- Reduce('+', faces.train.inputs.cleaned) / length(faces.train.inputs.cleaned)
image(avg.face)
faces.train.with.labels <- cbind(faces.train.inputs,faces.train.label)
colnames(faces.train.with.labels)
dim(faces.train.with.labels)
tail(colnames(faces.train.with.labels))
tail(rownames(faces.train.with.labels))
tail(colnames(faces.train.with.labels))
colnames(faces.train.with.labels)[length(colnames(faces.train.with.labels))]
dim(faces.train.inputs)
dim(faces.train.label)
dim(as.vector(faces.train.label))
dim(as.numeric(faces.train.label))
length(as.numeric(faces.train.label))
faces.train.with.labels <- cbind(faces.train.inputs,actual=as.numeric(faces.train.label))
dim(faces.train.with.labels)
dim(faces.train.inputs)
tail(colnames(faces.train.with.labels))
classes2 <- knn(faces.train.with.labels,faces.test.inputs,cl="actual",k =4)
??knn
library(class)
?knn
classes2 <- knn(faces.train.with.labels,faces.test.inputs,cl="actual",k =4)
classes2 <- knn(faces.train.inputs,faces.test.inputs,cl=faces.train.label,k =4)
classes2 <- knn(faces.train.inputs,faces.test.inputs,cl=as.numeric(faces.train.label),k =4)
classes2
k.nearest.neighbours <- function(training.data.matrix, training.data.labels, testing.data.matrix,
K = 10, distance.type = "squared_euclidean"){
# Make sure all the data is numeric
training.data.matrix <- data.matrix(training.data.matrix)
testing.data.matrix <- data.matrix(testing.data.matrix)
training.data.labels <- as.numeric(training.data.labels)
# Initialise the list which will take the classifications
classifiers <- c()
# iterate through every row of the testing matrix
for (i in 1:dim(testing.data.matrix)[1]){
# compute the distance of this row of the testing matrix to every other row in the training set
all.distances <- apply(training.data.matrix, MARGIN=1, function(x) distance(rbind(testing.data.matrix[i,],x),method=distance.type))
# sort these distances in increasing order.
sorted.distances <- sort(all.distances,index.return=TRUE)
# Look at the k closest rows in the training set to this testing row. Whichever classification comes
# up the most - is the classification we will give this particular row.
classification <- sort(tabulate(training.data.labels[sorted.distances$ix[1:K]]), index.return=TRUE,decreasing = TRUE)$ix[1]
# add it to the list of classifiers
classifiers <- c(classifiers,classification)
}
return(classifiers)
}
classes <- k.nearest.neighbours(training.data.matrix = faces.train.inputs,
training.data.labels = faces.train.label,
testing.data.matrix = faces.test.inputs,K=4)
all(classes == classes2)
classes
classes2
classes2 <- knn(faces.train.inputs,faces.test.inputs,cl=as.numeric(faces.train.label),k =4,l=3)
classes2
classes
all(classes[!is.na(classes2)] == classes2[!is.na(classes2)])
classes2[!is.na(classes2)]
classes2 <- knn(faces.train.inputs,faces.test.inputs,cl=as.numeric(faces.train.label),k =4,l=2)
all(classes[!is.na(classes2)] == classes2[!is.na(classes2)])
classes2
classes2 <- knn(faces.train.inputs,faces.test.inputs,cl=as.numeric(faces.train.label),k =5,l=4)
classes <- k.nearest.neighbours(training.data.matrix = faces.train.inputs,
training.data.labels = faces.train.label,
testing.data.matrix = faces.test.inputs,K=5)
all(classes[!is.na(classes2)] == classes2[!is.na(classes2)])
classes2
classes2 <- knn(faces.train.inputs,faces.test.inputs,cl=as.numeric(faces.train.label),k =5,l=3)
classes2
all(classes[!is.na(classes2)] == classes2[!is.na(classes2)])
classes2 <- knn(faces.train.inputs,faces.test.inputs,cl=as.numeric(faces.train.label),k =5,l=2)
all(classes[!is.na(classes2)] == classes2[!is.na(classes2)])
classes2
classes2 <- knn(faces.train.inputs,faces.test.inputs,cl=as.numeric(faces.train.label),k =5,l=3)
classes2
knn
VR_knn
k.nearest.neighbours <- function(training.data.matrix, training.data.labels, testing.data.matrix,
K = 10, distance.type = "squared_euclidean"){
# Make sure all the data is numeric
training.data.matrix <- data.matrix(training.data.matrix)
testing.data.matrix <- data.matrix(testing.data.matrix)
training.data.labels <- as.numeric(training.data.labels)
# Initialise the list which will take the classifications
classifiers <- c()
# iterate through every row of the testing matrix
for (i in 1:dim(testing.data.matrix)[1]){
# compute the distance of this row of the testing matrix to every other row in the training set
all.distances <- apply(training.data.matrix, MARGIN=1, function(x) distance(rbind(testing.data.matrix[i,],x),method=distance.type))
# sort these distances in increasing order.
sorted.distances <- sort(all.distances,index.return=TRUE)
# Look at the k closest rows in the training set to this testing row. Whichever classification comes
# up the most - is the classification we will give this particular row.
all.counts <- tabulate(training.data.labels[sorted.distances$ix[1:K]])
sorted.counts <- sort(all.counts, index.return=TRUE, decreasing=TRUE)
browser()
classification <- sort(all.counts, index.return=TRUE,decreasing = TRUE)$ix[1]
# add it to the list of classifiers
classifiers <- c(classifiers,classification)
}
return(classifiers)
}
classes <- k.nearest.neighbours(training.data.matrix = faces.train.inputs,
training.data.labels = faces.train.label,
testing.data.matrix = faces.test.inputs,K=5)
sorted.counts
all.counts
K
dim(all.distances)
length(all.distances)
length(sorted.distances)
sorted.distances$x
1:K
all.counts
length(all.counts)
training.data.labels[sorted.distances$ix[1:K]]
sorted.counts <- sort(all.counts, index.return=TRUE, decreasing=TRUE)
sorted.counts
?sample
sample(1:50,size=1)
sample(1:50,size=1)
sample(1:50,size=1)
sample(1:50,size=1)
k.nearest.neighbours <- function(training.data.matrix, training.data.labels, testing.data.matrix,
K = 10, distance.type = "squared_euclidean"){
# Make sure all the data is numeric
training.data.matrix <- data.matrix(training.data.matrix)
testing.data.matrix <- data.matrix(testing.data.matrix)
training.data.labels <- as.numeric(training.data.labels)
# Initialise the list which will take the classifications
classifiers <- c()
# iterate through every row of the testing matrix
for (i in 1:dim(testing.data.matrix)[1]){
# compute the distance of this row of the testing matrix to every other row in the training set
all.distances <- apply(training.data.matrix, MARGIN=1, function(x) distance(rbind(testing.data.matrix[i,],x),method=distance.type))
# sort these distances in increasing order.
sorted.distances <- sort(all.distances,index.return=TRUE)
# Look at the k closest rows in the training set to this testing row. Whichever classification comes
# up the most - is the classification we will give this particular row.
all.counts <- tabulate(training.data.labels[sorted.distances$ix[1:K]])
sorted.counts <- sort(all.counts, index.return=TRUE, decreasing=TRUE)
## inserted code
max.votes <- which(sorted.counts$x == sorted.counts$x[1])
if (length(max.votes) > 1){
# select a random number from 1:length(max.votes)
voter <- sample(1:length(max.votes),size=1)
} else {
voter <- 1
}
classification <- sorted.counts$ix[voter]
##
#classification <- sort(all.counts, index.return=TRUE,decreasing = TRUE)$ix[1]
# add it to the list of classifiers
classifiers <- c(classifiers,classification)
}
return(classifiers)
}
classes <- k.nearest.neighbours(training.data.matrix = faces.train.inputs,
training.data.labels = faces.train.label,
testing.data.matrix = faces.test.inputs,K=5)
classes
all(classes[!is.na(classes2)] == classes2[!is.na(classes2)])
classes2
classes
dimensions <- c(5,10,50)
single.face <- 1
means <- as.vector(avg.face)
for (i in dimensions){
eigenbasis <- find.pca.basis(i,faces.train.inputs)
projection.vals <- t(as.numeric(faces.train.inputs[single.face,]) - means) %*% eigenbasis
projection.vector <- eigenbasis %*% as.numeric(as.list(projection.vals))
# the mean in this basis...
projection.vals.mean <- t(as.numeric(faces.train.inputs[single.face,]) - means) %*% eigenbasis
image(matrix(projection.vector, nrow = 112),useRaster=TRUE, axes=FALSE)
}
find.pca.basis <- function(M,X, return.full.results = FALSE){
n <- dim(X)[1] # The number of images
# Turn the input data into a matrix and transpose it
X.data.matrix <- data.matrix(t(X))
# Centralise the data matrix
means <- rowMeans(X.data.matrix) # calculate row means
data.matrix.centralised <- X.data.matrix - means %*% t(rep(1,n)) # and subtract
# Calculate the covariance matrix as defined in lectures
covariance.matrix <- (data.matrix.centralised %*% t(data.matrix.centralised)) / n
# Now I need to compute the first M eigenvectors/ eigenvalues using the R package rARPACK
results <- eigs_sym(covariance.matrix,k=M,which="LM")
if (return.full.results){
return(results)
} else{
return(results$vectors)
}
}
dimensions <- c(5,10,50)
single.face <- 1
means <- as.vector(avg.face)
for (i in dimensions){
eigenbasis <- find.pca.basis(i,faces.train.inputs)
projection.vals <- t(as.numeric(faces.train.inputs[single.face,]) - means) %*% eigenbasis
projection.vector <- eigenbasis %*% as.numeric(as.list(projection.vals))
# the mean in this basis...
projection.vals.mean <- t(as.numeric(faces.train.inputs[single.face,]) - means) %*% eigenbasis
image(matrix(projection.vector, nrow = 112),useRaster=TRUE, axes=FALSE)
}
eigenbasis <- find.pca.basis(5,faces.train.inputs)
par(mfrow=c(2,3))
for (i in 1:5){
image(matrix(eigenbasis[,i], nrow = 112),useRaster=TRUE, axes=FALSE)
}
par(mfrow=c(1,1))
?image
par(mfrow=c(2,3))
for (i in 1:5){
image(matrix(eigenbasis[,i], nrow = 112),useRaster=TRUE, axes=FALSE,main=i)
}
par(mfrow=c(1,1))
# initialise the dimensions and face used.
dimensions <- c(5,10,50)
single.face <- 1
means <- as.vector(avg.face) # convert the mean face back into a vector
# iterate through the dimensions considered
for (i in dimensions){
# compute the eigenbasis using the function created
eigenbasis <- find.pca.basis(i,faces.train.inputs)
# calculate the projection values in this pca basis
projection.vals <- t(as.numeric(faces.train.inputs[single.face,]) - means)
%*% eigenbasis
# calculate the actual face in this basis
projection.vector <- eigenbasis %*% as.numeric(as.list(projection.vals))
# carry out the plot
image(matrix(projection.vector, nrow = 112),useRaster=TRUE, axes=FALSE)
}
par(mfrow=c(1,1))
# initialise the dimensions and face used.
dimensions <- c(5,10,50)
single.face <- 1
means <- as.vector(avg.face) # convert the mean face back into a vector
# iterate through the dimensions considered
for (i in dimensions){
# compute the eigenbasis using the function created
eigenbasis <- find.pca.basis(i,faces.train.inputs)
# calculate the projection values in this pca basis
projection.vals <- t(as.numeric(faces.train.inputs[single.face,]) - means)
%*% eigenbasis
# calculate the actual face in this basis
projection.vector <- eigenbasis %*% as.numeric(as.list(projection.vals))
# carry out the plot
image(matrix(projection.vector, nrow = 112),useRaster=TRUE, axes=FALSE)
}
means <- as.vector(avg.face) # convert the mean face back into a vector
means
image(avg.face)
projection.vals <- t(as.numeric(faces.train.inputs[single.face,]) - means) %*% eigenbasis
projection.vector <- eigenbasis %*% as.numeric(as.list(projection.vals))
image(matrix(projection.vector, nrow = 112),useRaster=TRUE, axes=FALSE)
# initialise the dimensions and face used.
dimensions <- c(5,10,50)
single.face <- 1
means <- as.vector(avg.face) # convert the mean face back into a vector
# iterate through the dimensions considered
for (i in dimensions){
# compute the eigenbasis using the function created
eigenbasis <- find.pca.basis(i,faces.train.inputs)
# calculate the projection values in this pca basis
projection.vals <- t(as.numeric(faces.train.inputs[single.face,]) - means) %*% eigenbasis
# calculate the actual face in this basis
projection.vector <- eigenbasis %*% as.numeric(as.list(projection.vals))
# carry out the plot
image(matrix(projection.vector, nrow = 112),useRaster=TRUE, axes=FALSE)
}
dimensions <- c(5,10,50)
single.face <- 1
means <- as.vector(avg.face) # convert the mean face back into a vector
# iterate through the dimensions considered
for (i in dimensions){
# compute the eigenbasis using the function created
eigenbasis <- find.pca.basis(i,faces.train.inputs)
# calculate the projection values in this pca basis
projection.vals <- t(as.numeric(faces.train.inputs[single.face,]) -
means) %*% eigenbasis
# calculate the actual face in this basis
projection.vector <- eigenbasis %*% as.numeric(as.list(projection.vals))
# carry out the plot
image(matrix(projection.vector, nrow = 112),useRaster=TRUE,
axes=FALSE,main=paste("dimension",i,sep=""))
}
# initialise the dimensions and face used.
dimensions <- c(5,10,50)
single.face <- 1
means <- as.vector(avg.face) # convert the mean face back into a vector
# iterate through the dimensions considered
for (i in dimensions){
# compute the eigenbasis using the function created
eigenbasis <- find.pca.basis(i,faces.train.inputs)
# calculate the projection values in this pca basis
projection.vals <- t(as.numeric(faces.train.inputs[single.face,]) -
means) %*% eigenbasis
# calculate the actual face in this basis
projection.vector <- eigenbasis %*% as.numeric(as.list(projection.vals))
# carry out the plot
image(matrix(projection.vector, nrow = 112),useRaster=TRUE,
axes=FALSE,main=paste("dimension",i,sep=" "))
}
# and here's the original image
image(matrix(as.numeric(faces.train.inputs[single.face,]), nrow = 112),
useRaster=TRUE, axes=FALSE, main="Original Image")
?eigs_sym
# First I calculate the full pca basis (and all the assosciated eigenvalues)
full.results <- find.pca.basis(320,faces.train.inputs,return.full.results = TRUE)
# the mean square error is equal to the total variance minus the sum of the
# eigenvalues for components not used
mses <- (cumsum(full.results$values)[length(full.results$values)] -
cumsum(full.results$values) )
plot(mses,xlab="Dimensionality of PCA",ylab="Mean Square Error")
?distance
## compute the eigenbasis then move all of the data into this PCA space
eigenbasis <- find.pca.basis(150,faces.train.inputs)
faces.train.new.basis <- lapply(X = c(1:320),
FUN=function(x) t(as.numeric(faces.train.inputs[x,])
- means) %*% eigenbasis)
faces.train.new.basis <- do.call("rbind",faces.train.new.basis)
faces.test.new.basis <- lapply(X = c(1:80),
FUN=function(x) t(as.numeric(faces.test.inputs[x,]) -
means) %*% eigenbasis)
faces.test.new.basis <- do.call("rbind",faces.test.new.basis)
# initialise the list of accuracies
accuracy.sor.list <- c()
accuracy.man.list <- c()
accuracy.sq.list <- c()
for (i in 1:8){ # these are the values of k we will use
# calculate the classes for each distance type
classes.sor <- k.nearest.neighbours(training.data.matrix = faces.train.new.basis ,
training.data.labels = faces.train.label,
testing.data.matrix = faces.test.new.basis,K=i,
distance.type = "sorensen")
classes.man <- k.nearest.neighbours(training.data.matrix = faces.train.new.basis,
training.data.labels = faces.train.label,
testing.data.matrix = faces.test.new.basis,K=i,
distance.type = "manhattan")
classes.sq <- k.nearest.neighbours(training.data.matrix = faces.train.new.basis,
training.data.labels = faces.train.label,
testing.data.matrix = faces.test.new.basis,K=i,
distance.type = "squared_euclidean")
classes.actual <- as.integer(faces.test.label)
## add the accuracy to the list of accuracies..
accuracy.sor <- length(which(classes.euc == classes.actual)) /
length(classes.actual)
accuracy.sor.list <- c(accuracy.sor.list,accuracy.sor)
accuracy.man <-length(which(classes.man == classes.actual)) /
length(classes.actual)
accuracy.man.list <- c(accuracy.man.list,accuracy.man)
accuracy.sq <- length(which(classes.squaredeuc == classes.actual)) /
length(classes.actual)
accuracy.sq.list <- c(accuracy.sq.list,accuracy.sq)
}
## compute the eigenbasis then move all of the data into this PCA space
eigenbasis <- find.pca.basis(150,faces.train.inputs)
faces.train.new.basis <- lapply(X = c(1:320),
FUN=function(x) t(as.numeric(faces.train.inputs[x,])
- means) %*% eigenbasis)
faces.train.new.basis <- do.call("rbind",faces.train.new.basis)
faces.test.new.basis <- lapply(X = c(1:80),
FUN=function(x) t(as.numeric(faces.test.inputs[x,]) -
means) %*% eigenbasis)
faces.test.new.basis <- do.call("rbind",faces.test.new.basis)
# initialise the list of accuracies
accuracy.sor.list <- c()
accuracy.man.list <- c()
accuracy.sq.list <- c()
for (i in 1:8){ # these are the values of k we will use
# calculate the classes for each distance type
classes.sor <- k.nearest.neighbours(training.data.matrix = faces.train.new.basis ,
training.data.labels = faces.train.label,
testing.data.matrix = faces.test.new.basis,K=i,
distance.type = "sorensen")
classes.man <- k.nearest.neighbours(training.data.matrix = faces.train.new.basis,
training.data.labels = faces.train.label,
testing.data.matrix = faces.test.new.basis,K=i,
distance.type = "manhattan")
classes.sq <- k.nearest.neighbours(training.data.matrix = faces.train.new.basis,
training.data.labels = faces.train.label,
testing.data.matrix = faces.test.new.basis,K=i,
distance.type = "squared_euclidean")
classes.actual <- as.integer(faces.test.label)
## add the accuracy to the list of accuracies..
accuracy.sor <- length(which(classes.sor == classes.actual)) /
length(classes.actual)
accuracy.sor.list <- c(accuracy.sor.list,accuracy.sor)
accuracy.man <-length(which(classes.man == classes.actual)) /
length(classes.actual)
accuracy.man.list <- c(accuracy.man.list,accuracy.man)
accuracy.sq <- length(which(classes.sq == classes.actual)) /
length(classes.actual)
accuracy.sq.list <- c(accuracy.sq.list,accuracy.sq)
}
plot(accuracy.sor.list)
plot(accuracy.man.list)
plot(accuracy.sq.list)
?matplot
fullmat <- cbind(accuracy.sor.list,accuracy.man.list,accuracy.sq.list)
dim(fullmat)
matplot(fullmat, type = c("b"),pch=1,col = 1:3) #plot
legend("topleft", legend = 1:3, col=1:3, pch=1)
legend("right", legend = 1:3, col=1:3, pch=1)
fullmat <- cbind(accuracy.sor.list,accuracy.man.list,accuracy.sq.list)
matplot(fullmat, type = c("b"),pch=1,col = 1:3) #plot
legend("right", legend = 1:3, col=1:3, pch=1)
fullmat <- cbind(sorensen=accuracy.sor.list,manhattan=accuracy.man.list,euclidean=accuracy.sq.list)
matplot(fullmat, type = c("b"),pch=1,col = 1:3) #plot
legend("right", legend = 1:3, col=1:3, pch=1)
fullmat <- cbind(sorensen=accuracy.sor.list,manhattan=accuracy.man.list,euclidean=accuracy.sq.list)
matplot(fullmat, type = c("b"),pch=1,col = 1:3) #plot
legend("right", legend = colnames(fullmat), col=1:3, pch=1)
## TRY pca preprocessing
## compute the eigenbasis then move all of the data into this PCA space
eigenbasis <- find.pca.basis(100,faces.train.inputs)
faces.train.new.basis <- lapply(X = c(1:320),
FUN=function(x) t(as.numeric(faces.train.inputs[x,])
- means) %*% eigenbasis)
faces.train.new.basis <- do.call("rbind",faces.train.new.basis)
faces.test.new.basis <- lapply(X = c(1:80),
FUN=function(x) t(as.numeric(faces.test.inputs[x,]) -
means) %*% eigenbasis)
faces.test.new.basis <- do.call("rbind",faces.test.new.basis)
# initialise the list of accuracies
accuracy.sor.list <- c()
accuracy.man.list <- c()
accuracy.sq.list <- c()
for (i in 1:8){ # these are the values of k we will use
# calculate the classes for each distance type
classes.sor <- k.nearest.neighbours(training.data.matrix = faces.train.new.basis ,
training.data.labels = faces.train.label,
testing.data.matrix = faces.test.new.basis,K=i,
distance.type = "sorensen")
classes.man <- k.nearest.neighbours(training.data.matrix = faces.train.new.basis,
training.data.labels = faces.train.label,
testing.data.matrix = faces.test.new.basis,K=i,
distance.type = "manhattan")
classes.sq <- k.nearest.neighbours(training.data.matrix = faces.train.new.basis,
training.data.labels = faces.train.label,
testing.data.matrix = faces.test.new.basis,K=i,
distance.type = "squared_euclidean")
classes.actual <- as.integer(faces.test.label)
## add the accuracy to the list of accuracies..
accuracy.sor <- 100*length(which(classes.sor == classes.actual)) /
length(classes.actual)
accuracy.sor.list <- c(accuracy.sor.list,accuracy.sor)
accuracy.man <- 100*length(which(classes.man == classes.actual)) /
length(classes.actual)
accuracy.man.list <- c(accuracy.man.list,accuracy.man)
accuracy.sq <- 100*length(which(classes.sq == classes.actual)) /
length(classes.actual)
accuracy.sq.list <- c(accuracy.sq.list,accuracy.sq)
}
## now check and plot them
# plot all three on same graph
fullmat <- cbind(sorensen=accuracy.sor.list,manhattan=accuracy.man.list,euclidean=accuracy.sq.list)
matplot(fullmat, type = c("b"),pch=1,col = 1:3,ylab="accuracy",xlab="k") #plot
legend("right", legend = colnames(fullmat), col=1:3, pch=1)
accuracy.sq.list[1]
92*112
?philentropy
??philentropy
